---
# Source: opik/charts/mysql/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: opik-mysql
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: opik
      app.kubernetes.io/managed-by: Helm
      app.kubernetes.io/name: mysql
      app.kubernetes.io/version: 8.4.1
      helm.sh/chart: mysql-11.1.9
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    - ports:
        - port: 3306
        - port: 3306
---
# Source: opik/charts/redis/templates/networkpolicy.yaml
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: opik-redis
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
spec:
  podSelector:
    matchLabels:
      app.kubernetes.io/instance: opik
      app.kubernetes.io/name: redis
  policyTypes:
    - Ingress
    - Egress
  egress:
    - {}
  ingress:
    # Allow inbound connections
    - ports:
        - port: 6379
---
# Source: opik/templates/networkpolicies.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: opik-python-backend-egress
spec:
  podSelector:
    matchLabels:
      component: opik-python-backend
  policyTypes:
    - Egress
  egress:
    # Allow DNS traffic to kube-dns/coredns
    - to:
      - namespaceSelector:
          matchLabels:
            kubernetes.io/metadata.name: kube-system
        podSelector:
          matchLabels:
            k8s-app: kube-dns
      ports:
        - protocol: UDP
          port: 53
        - protocol: TCP
          port: 53
    # Allow traffic to external destinations (outside the cluster)
    - to:
      - ipBlock:
          cidr: 0.0.0.0/0
          except:
          - 10.0.0.0/8
          - 100.64.0.0/10
          - 172.16.0.0/12
          - 192.0.0.0/24
          - 198.18.0.0/15
          - 192.168.0.0/16
---
# Source: opik/charts/mysql/templates/primary/pdb.yaml
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: opik-mysql
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
    app.kubernetes.io/component: primary
spec:
  maxUnavailable: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: opik
      app.kubernetes.io/name: mysql
      app.kubernetes.io/component: primary
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ServiceAccount-clickhouse-operator.yaml
# Template Parameters:
#
# COMMENT=
# NAMESPACE=kube-system
# NAME=clickhouse-operator
#
# Setup ServiceAccount
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opik-altinity-clickhouse-operator
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
  annotations: 
    {}

# Template Parameters:
#
# NAMESPACE=kube-system
# COMMENT=#
# ROLE_KIND=ClusterRole
# ROLE_NAME=clickhouse-operator-kube-system
# ROLE_BINDING_KIND=ClusterRoleBinding
# ROLE_BINDING_NAME=clickhouse-operator-kube-system
#
---
# Source: opik/charts/mysql/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opik-mysql
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
automountServiceAccountToken: false
secrets:
  - name: opik-mysql
---
# Source: opik/charts/redis/templates/master/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: false
metadata:
  name: opik-redis-master
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
---
# Source: opik/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opik-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
---
# Source: opik/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opik-frontend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
---
# Source: opik/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opik-python-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/Secret-clickhouse-operator.yaml
#
# Template parameters available:
#   NAMESPACE=kube-system
#   COMMENT=
#   OPERATOR_VERSION=0.23.7
#   CH_USERNAME_SECRET_PLAIN=clickhouse_operator
#   CH_PASSWORD_SECRET_PLAIN=clickhouse_operator_password
#
apiVersion: v1
kind: Secret
metadata:
  name: opik-altinity-clickhouse-operator
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
type: Opaque
data:
  username: Y2xpY2tob3VzZV9vcGVyYXRvcg==
  password: Y2xpY2tob3VzZV9vcGVyYXRvcl9wYXNzd29yZA==
---
# Source: opik/charts/mysql/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: opik-mysql
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
type: Opaque
data:
  mysql-root-password: "b3Bpaw=="
  mysql-password: "clMyNTBackhIbw=="
---
# Source: opik/charts/redis/templates/secret.yaml
apiVersion: v1
kind: Secret
metadata:
  name: opik-redis
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
type: Opaque
data:
  redis-password: "d0ZTdUpYOW5EQmRDYTI1c0taRzdiaA=="
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-confd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-confd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-altinity-clickhouse-operator-confd-files
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
data: 
  null
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-configd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-configd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-altinity-clickhouse-operator-configd-files
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
data: 
  01-clickhouse-01-listen.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <!-- Listen wildcard address to allow accepting connections from other containers and host network. -->
        <listen_host>::</listen_host>
        <listen_host>0.0.0.0</listen_host>
        <listen_try>1</listen_try>
    </yandex>
  01-clickhouse-02-logger.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <logger>
            <!-- Possible levels: https://github.com/pocoproject/poco/blob/devel/Foundation/include/Poco/Logger.h#L439 -->
            <level>debug</level>
            <log>/var/log/clickhouse-server/clickhouse-server.log</log>
            <errorlog>/var/log/clickhouse-server/clickhouse-server.err.log</errorlog>
            <size>1000M</size>
            <count>10</count>
            <!-- Default behavior is autodetection (log to console if not daemon mode and is tty) -->
            <console>1</console>
        </logger>
    </yandex>
  01-clickhouse-03-query_log.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <query_log replace="1">
            <database>system</database>
            <table>query_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </query_log>
        <query_thread_log remove="1"/>
    </yandex>
  01-clickhouse-04-part_log.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <part_log replace="1">
            <database>system</database>
            <table>part_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </part_log>
    </yandex>
  01-clickhouse-05-trace_log.xml: |-
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
        <trace_log replace="1">
            <database>system</database>
            <table>trace_log</table>
            <engine>Engine = MergeTree PARTITION BY event_date ORDER BY event_time TTL event_date + interval 30 day</engine>
            <flush_interval_milliseconds>7500</flush_interval_milliseconds>
        </trace_log>
    </yandex>
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-altinity-clickhouse-operator-files
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
data: 
  config.yaml: |-
    annotation:
      exclude: []
      include: []
    clickhouse:
      access:
        password: ""
        port: 8123
        rootCA: ""
        scheme: auto
        secret:
          name: 'opik-altinity-clickhouse-operator'
          namespace: ""
        timeouts:
          connect: 1
          query: 4
        username: ""
      configuration:
        file:
          path:
            common: config.d
            host: conf.d
            user: users.d
        network:
          hostRegexpTemplate: (chi-{chi}-[^.]+\d+-\d+|clickhouse\-{chi})\.{namespace}\.svc\.cluster\.local$
        user:
          default:
            networksIP:
            - ::1
            - 127.0.0.1
            password: default
            profile: default
            quota: default
      configurationRestartPolicy:
        rules:
        - rules:
          - settings/*: "yes"
          - settings/access_control_path: "no"
          - settings/dictionaries_config: "no"
          - settings/max_server_memory_*: "no"
          - settings/max_*_to_drop: "no"
          - settings/max_concurrent_queries: "no"
          - settings/models_config: "no"
          - settings/user_defined_executable_functions_config: "no"
          - settings/logger/*: "no"
          - settings/macros/*: "no"
          - settings/remote_servers/*: "no"
          - settings/user_directories/*: "no"
          - zookeeper/*: "yes"
          - files/*.xml: "yes"
          - files/config.d/*.xml: "yes"
          - files/config.d/*dict*.xml: "no"
          - profiles/default/background_*_pool_size: "yes"
          - profiles/default/max_*_for_server: "yes"
          version: '*'
        - rules:
          - settings/logger: "yes"
          version: 21.*
      metrics:
        timeouts:
          collect: 9
    label:
      appendScope: "no"
      exclude: []
      include: []
    logger:
      alsologtostderr: "false"
      log_backtrace_at: ""
      logtostderr: "true"
      stderrthreshold: ""
      v: "1"
      vmodule: ""
    pod:
      terminationGracePeriod: 30
    reconcile:
      host:
        wait:
          exclude: true
          include: false
          queries: true
      runtime:
        reconcileCHIsThreadsNumber: 10
        reconcileShardsMaxConcurrencyPercent: 50
        reconcileShardsThreadsNumber: 5
      statefulSet:
        create:
          onFailure: ignore
        update:
          onFailure: abort
          pollInterval: 5
          timeout: 300
    statefulSet:
      revisionHistoryLimit: 0
    template:
      chi:
        path: templates.d
        policy: ApplyOnNextReconcile
    watch:
      namespaces: []
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-templatesd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-templatesd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-altinity-clickhouse-operator-templatesd-files
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
data: 
  001-templates.json.example: |
    {
      "apiVersion": "clickhouse.altinity.com/v1",
      "kind": "ClickHouseInstallationTemplate",
      "metadata": {
        "name": "01-default-volumeclaimtemplate"
      },
      "spec": {
        "templates": {
          "volumeClaimTemplates": [
            {
              "name": "chi-default-volume-claim-template",
              "spec": {
                "accessModes": [
                  "ReadWriteOnce"
                ],
                "resources": {
                  "requests": {
                    "storage": "2Gi"
                  }
                }
              }
            }
          ],
          "podTemplates": [
            {
              "name": "chi-default-oneperhost-pod-template",
              "distribution": "OnePerHost",
              "spec": {
                "containers" : [
                  {
                    "name": "clickhouse",
                    "image": "clickhouse/clickhouse-server:23.8",
                    "ports": [
                      {
                        "name": "http",
                        "containerPort": 8123
                      },
                      {
                        "name": "client",
                        "containerPort": 9000
                      },
                      {
                        "name": "interserver",
                        "containerPort": 9009
                      }
                    ]
                  }
                ]
              }
            }
          ]
        }
      }
    }
  default-pod-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-oneperhost-pod-template"
    spec:
      templates:
        podTemplates:
          - name: default-oneperhost-pod-template
            distribution: "OnePerHost"
  default-storage-template.yaml.example: |
    apiVersion: "clickhouse.altinity.com/v1"
    kind: "ClickHouseInstallationTemplate"
    metadata:
      name: "default-storage-template-2Gi"
    spec:
      templates:
        volumeClaimTemplates:
          - name: default-storage-template-2Gi
            spec:
              accessModes:
                - ReadWriteOnce
              resources:
                requests:
                  storage: 2Gi
  readme: Templates in this folder are packaged with an operator and available via 'useTemplate'
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ConfigMap-etc-clickhouse-operator-usersd-files.yaml
# Template Parameters:
#
# NAME=etc-clickhouse-operator-usersd-files
# NAMESPACE=kube-system
# COMMENT=
#
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-altinity-clickhouse-operator-usersd-files
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
data: 
  01-clickhouse-operator-profile.xml: |
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <!--
    #
    # Template parameters available:
    #
    -->
    <yandex>
        <!-- clickhouse-operator user is generated by the operator based on config.yaml in runtime -->
        <profiles>
            <clickhouse_operator>
                <log_queries>0</log_queries>
                <skip_unavailable_shards>1</skip_unavailable_shards>
                <http_connection_timeout>10</http_connection_timeout>
                <max_concurrent_queries_for_all_users>0</max_concurrent_queries_for_all_users>
                <os_thread_priority>0</os_thread_priority>
            </clickhouse_operator>
        </profiles>
    </yandex>
  02-clickhouse-default-profile.xml: |-
    <!-- IMPORTANT -->
    <!-- This file is auto-generated -->
    <!-- Do not edit this file - all changes would be lost -->
    <!-- Edit appropriate template in the following folder: -->
    <!-- deploy/builder/templates-config -->
    <!-- IMPORTANT -->
    <yandex>
      <profiles>
        <default>
          <os_thread_priority>2</os_thread_priority>
          <log_queries>1</log_queries>
          <connect_timeout_with_failover_ms>1000</connect_timeout_with_failover_ms>
          <distributed_aggregation_memory_efficient>1</distributed_aggregation_memory_efficient>
          <parallel_view_processing>1</parallel_view_processing>
          <do_not_merge_across_partitions_select_final>1</do_not_merge_across_partitions_select_final>
          <load_balancing>nearest_hostname</load_balancing>
          <prefer_localhost_replica>0</prefer_localhost_replica>
          <!-- materialize_ttl_recalculate_only>1</materialize_ttl_recalculate_only> 21.10 and above -->
        </default>
      </profiles>
    </yandex>
---
# Source: opik/charts/mysql/templates/primary/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-mysql
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
    app.kubernetes.io/component: primary
data:
  my.cnf: |-
    [mysqld]
    authentication_policy='* ,,'
    skip-name-resolve
    explicit_defaults_for_timestamp
    basedir=/opt/bitnami/mysql
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    port=3306
    mysqlx=0
    mysqlx_port=33060
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    datadir=/bitnami/mysql/data
    tmpdir=/opt/bitnami/mysql/tmp
    max_allowed_packet=16M
    bind-address=*
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
    log-error=/opt/bitnami/mysql/logs/mysqld.log
    character-set-server=UTF8
    slow_query_log=0
    long_query_time=10.0
    
    [client]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    default-character-set=UTF8
    plugin_dir=/opt/bitnami/mysql/lib/plugin
    
    [manager]
    port=3306
    socket=/opt/bitnami/mysql/tmp/mysql.sock
    pid-file=/opt/bitnami/mysql/tmp/mysqld.pid
---
# Source: opik/charts/mysql/templates/primary/initialization-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-mysql-init-scripts
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
    app.kubernetes.io/component: primary
data:
  createdb.sql: |-
    CREATE DATABASE IF NOT EXISTS opik DEFAULT CHARACTER SET utf8 DEFAULT COLLATE utf8_general_ci;
    CREATE USER IF NOT EXISTS 'opik'@'%' IDENTIFIED BY 'opik';
    GRANT ALL ON `opik`.* TO 'opik'@'%';
    FLUSH PRIVILEGES;
---
# Source: opik/charts/redis/templates/configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-redis-configuration
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
data:
  redis.conf: |-
    # User-supplied common configuration:
    # Enable AOF https://redis.io/topics/persistence#append-only-file
    appendonly yes
    # Disable RDB persistence, AOF persistence already enabled.
    save ""
    # End of common configuration
  master.conf: |-
    dir /data
    # User-supplied master configuration:
    maxmemory 105M
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of master configuration
  replica.conf: |-
    dir /data
    # User-supplied replica configuration:
    rename-command FLUSHDB ""
    rename-command FLUSHALL ""
    # End of replica configuration
---
# Source: opik/charts/redis/templates/extra-list.yaml
apiVersion: v1
data:
  start-master.sh: |
    #!/usr/bin/dumb-init /bin/bash
    ### docker entrypoint script, for starting redis stack
    BASEDIR=/opt/redis-stack
    cd ${BASEDIR}
    CMD=${BASEDIR}/bin/redis-server
    if [ -z "${REDISEARCH_ARGS}" ]; then
    REDISEARCH_ARGS="MAXSEARCHRESULTS 10000 MAXAGGREGATERESULTS 10000"
    fi
    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ${CMD} \
    --port "${REDIS_PORT}" \
    --requirepass "${REDIS_PASSWORD}" \
    --masterauth "${REDIS_PASSWORD}" \
    --include "/opt/bitnami/redis/etc/redis.conf" \
    --include "/opt/bitnami/redis/etc/master.conf" \
    --loadmodule ${BASEDIR}/lib/redisearch.so ${REDISEARCH_ARGS} \
    --loadmodule ${BASEDIR}/lib/redistimeseries.so ${REDISTIMESERIES_ARGS} \
    --loadmodule ${BASEDIR}/lib/rejson.so ${REDISJSON_ARGS} \
    --loadmodule ${BASEDIR}/lib/redisbloom.so ${REDISBLOOM_ARGS}
kind: ConfigMap
metadata:
  name: bitnami-redis-stack-server-merged
---
# Source: opik/charts/redis/templates/health-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-redis-health
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
data:
  ping_readiness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_local.sh: |-
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    [[ -n "$REDIS_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h localhost \
        -p $REDIS_PORT \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ] && [ "$responseFirstWord" != "MASTERDOWN" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    if [ "$response" != "PONG" ]; then
      echo "$response"
      exit 1
    fi
  ping_liveness_master.sh: |-
    #!/bin/bash

    [[ -f $REDIS_MASTER_PASSWORD_FILE ]] && export REDIS_MASTER_PASSWORD="$(< "${REDIS_MASTER_PASSWORD_FILE}")"
    [[ -n "$REDIS_MASTER_PASSWORD" ]] && export REDISCLI_AUTH="$REDIS_MASTER_PASSWORD"
    response=$(
      timeout -s 15 $1 \
      redis-cli \
        -h $REDIS_MASTER_HOST \
        -p $REDIS_MASTER_PORT_NUMBER \
        ping
    )
    if [ "$?" -eq "124" ]; then
      echo "Timed out"
      exit 1
    fi
    responseFirstWord=$(echo $response | head -n1 | awk '{print $1;}')
    if [ "$response" != "PONG" ] && [ "$responseFirstWord" != "LOADING" ]; then
      echo "$response"
      exit 1
    fi
  ping_readiness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_readiness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_readiness_master.sh" $1 || exit_status=$?
    exit $exit_status
  ping_liveness_local_and_master.sh: |-
    script_dir="$(dirname "$0")"
    exit_status=0
    "$script_dir/ping_liveness_local.sh" $1 || exit_status=$?
    "$script_dir/ping_liveness_master.sh" $1 || exit_status=$?
    exit $exit_status
---
# Source: opik/charts/redis/templates/scripts-configmap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-redis-scripts
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
data:
  start-master.sh: |
    #!/bin/bash

    [[ -f $REDIS_PASSWORD_FILE ]] && export REDIS_PASSWORD="$(< "${REDIS_PASSWORD_FILE}")"
    if [[ -f /opt/bitnami/redis/mounted-etc/master.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/master.conf /opt/bitnami/redis/etc/master.conf
    fi
    if [[ -f /opt/bitnami/redis/mounted-etc/redis.conf ]];then
        cp /opt/bitnami/redis/mounted-etc/redis.conf /opt/bitnami/redis/etc/redis.conf
    fi
    ARGS=("--port" "${REDIS_PORT}")
    ARGS+=("--requirepass" "${REDIS_PASSWORD}")
    ARGS+=("--masterauth" "${REDIS_PASSWORD}")
    ARGS+=("--include" "/opt/bitnami/redis/etc/redis.conf")
    ARGS+=("--include" "/opt/bitnami/redis/etc/master.conf")
    exec redis-server "${ARGS[@]}"
---
# Source: opik/templates/configmap-backend.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
data:
  ANALYTICS_DB_DATABASE_NAME: "opik"
  ANALYTICS_DB_HOST: "clickhouse-opik-clickhouse"
  ANALYTICS_DB_MIGRATIONS_PASS: "opik"
  ANALYTICS_DB_MIGRATIONS_URL: "jdbc:clickhouse://clickhouse-opik-clickhouse:8123"
  ANALYTICS_DB_MIGRATIONS_USER: "opik"
  ANALYTICS_DB_PASS: "opik"
  ANALYTICS_DB_PORT: "8123"
  ANALYTICS_DB_PROTOCOL: "HTTP"
  ANALYTICS_DB_USERNAME: "opik"
  JAVA_OPTS: "-Dliquibase.propertySubstitutionEnabled=true -XX:+UseG1GC -XX:MaxRAMPercentage=80.0 -XX:MinRAMPercentage=75"
  OPIK_OTEL_SDK_ENABLED: "false"
  OTEL_EXPERIMENTAL_EXPORTER_OTLP_RETRY_ENABLED: "true"
  OTEL_EXPERIMENTAL_RESOURCE_DISABLED_KEYS: "process.command_args"
  OTEL_EXPORTER_OTLP_METRICS_DEFAULT_HISTOGRAM_AGGREGATION: "BASE2_EXPONENTIAL_BUCKET_HISTOGRAM"
  OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE: "delta"
  OTEL_PROPAGATORS: "tracecontext,baggage,b3"
  OTEL_VERSION: "2.12.0"
  REDIS_URL: "redis://:wFSuJX9nDBdCa25sKZG7bh@opik-redis-master:6379/"
  STATE_DB_DATABASE_NAME: "opik"
  STATE_DB_PASS: "opik"
  STATE_DB_PROTOCOL: "jdbc:mysql://"
  STATE_DB_URL: "opik-mysql:3306/opik?rewriteBatchedStatements=true"
  STATE_DB_USER: "opik"
---
# Source: opik/templates/configmap-backend.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-python-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
data:
  PYTHON_CODE_EXECUTOR_IMAGE_NAME: "opik-sandbox-executor-python"
  PYTHON_CODE_EXECUTOR_IMAGE_REGISTRY: "ghcr.io/comet-ml/opik"
  PYTHON_CODE_EXECUTOR_IMAGE_TAG: "latest"
---
# Source: opik/templates/configmap-frontend-nginx.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opik-frontend-nginx
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
data:
  default.conf: |
    gzip on;
    gzip_disable "msie6";
    gzip_vary on;
    gzip_proxied any;
    gzip_comp_level 6;
    gzip_buffers 16 8k;
    gzip_http_version 1.1;
    gzip_min_length 256;
    gzip_types text/plain text/css application/json application/x-javascript text/xml application/xml application/xml+rss text/javascript application/vnd.ms-fontobject application/x-font-ttf font/opentype image/svg+xml image/x-icon application/javascript;
    absolute_redirect off;
    real_ip_header      X-Forwarded-For;
    real_ip_recursive   on;
    set_real_ip_from    10.0.0.0/8;
    rewrite_log on;
    limit_req_log_level info;
    limit_req_status 429;
    map_hash_bucket_size 256;
    client_max_body_size 500M;
    resolver_timeout 3s;
    server_tokens off;
    proxy_socket_keepalive on;
    proxy_http_version 1.1;
    proxy_set_header Connection "";

    upstream backend {
      server opik-backend:8080;
      keepalive 16;
    }
    log_format logger-json escape=json '{ "body_bytes_sent": $body_bytes_sent, "http_referer": "$http_referer", "http_user_agent": "$http_user_agent", "remote_addr": "$remote_addr", "remote_user": "$remote_user", "request": "$request", "status": $status, "time_local": "$time_local", "x_forwarded_for": "$http_x_forwarded_for" }';
    
    access_log /dev/stdout logger-json;

    server {
        listen 5173 default_server;
        server_name localhost;

        location @api {
            rewrite /api/(.*) /$1  break;
            proxy_pass http://backend;
            proxy_redirect  off;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;

            proxy_read_timeout 90;
            proxy_connect_timeout 90;
            proxy_send_timeout 90;

            proxy_http_version 1.1;
            proxy_set_header Upgrade $http_upgrade;
            proxy_set_header Connection "upgrade";
        }
        location ^~ /api/ {
          try_files /dev/null @api;
        }

        location / {
            alias /usr/share/nginx/html/;

            try_files $uri $uri/ /index.html;

            if ($uri = /index.html) {
                add_header Cache-Control "no-cache";
            }
        }

    }
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ClusterRole-clickhouse-operator-kube-system.yaml
# Specifies either
#   ClusterRole
# or
#   Role
# to be bound to ServiceAccount.
# ClusterRole is namespace-less and must have unique name
# Role is namespace-bound
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: opik-altinity-clickhouse-operator
  #namespace: kube-system
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
  namespace: opik
rules:
  #
  # Core API group
  #
  - apiGroups:
      - ""
    resources:
      - configmaps
      - services
      - persistentvolumeclaims
      - secrets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  - apiGroups:
      - ""
    resources:
      - endpoints
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - ""
    resources:
      - events
    verbs:
      - create
  - apiGroups:
      - ""
    resources:
      - persistentvolumes
    verbs:
      - get
      - list
      - patch
      - update
      - watch
  - apiGroups:
      - ""
    resources:
      - pods
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - delete
  - apiGroups:
      - ""
    resources:
      - secrets
    verbs:
      - get
      - list
  #
  # apps.* resources
  #
  - apiGroups:
      - apps
    resources:
      - statefulsets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  - apiGroups:
      - apps
    resources:
      - replicasets
    verbs:
      - get
      - patch
      - update
      - delete
  # The operator deployment personally, identified by name
  - apiGroups:
      - apps
    resources:
      - deployments
    resourceNames:
      - opik-altinity-clickhouse-operator
    verbs:
      - get
      - patch
      - update
      - delete
  #
  # policy.* resources
  #
  - apiGroups:
      - policy
    resources:
      - poddisruptionbudgets
    verbs:
      - get
      - list
      - patch
      - update
      - watch
      - create
      - delete
  #
  # apiextensions
  #
  - apiGroups:
      - apiextensions.k8s.io
    resources:
      - customresourcedefinitions
    verbs:
      - get
      - list
  # clickhouse - related resources
  - apiGroups:
      - clickhouse.altinity.com
    #
    # The operators specific Custom Resources
    #

    resources:
      - clickhouseinstallations
    verbs:
      - get
      - list
      - watch
      - patch
      - update
      - delete
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallationtemplates
      - clickhouseoperatorconfigurations
    verbs:
      - get
      - list
      - watch
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations/finalizers
      - clickhouseinstallationtemplates/finalizers
      - clickhouseoperatorconfigurations/finalizers
    verbs:
      - update
  - apiGroups:
      - clickhouse.altinity.com
    resources:
      - clickhouseinstallations/status
      - clickhouseinstallationtemplates/status
      - clickhouseoperatorconfigurations/status
    verbs:
      - get
      - update
      - patch
      - create
      - delete
  # clickhouse-keeper - related resources
  - apiGroups:
      - clickhouse-keeper.altinity.com
    resources:
      - clickhousekeeperinstallations
    verbs:
      - get
      - list
      - watch
      - patch
      - update
      - delete
  - apiGroups:
      - clickhouse-keeper.altinity.com
    resources:
      - clickhousekeeperinstallations/finalizers
    verbs:
      - update
  - apiGroups:
      - clickhouse-keeper.altinity.com
    resources:
      - clickhousekeeperinstallations/status
    verbs:
      - get
      - update
      - patch
      - create
      - delete
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/ClusterRoleBinding-clickhouse-operator-kube-system.yaml
# Specifies either
#   ClusterRoleBinding between ClusterRole and ServiceAccount.
# or
#   RoleBinding between Role and ServiceAccount.
# ClusterRoleBinding is namespace-less and must have unique name
# RoleBinding is namespace-bound
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: opik-altinity-clickhouse-operator
  #namespace: kube-system
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
  namespace: opik
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: opik-altinity-clickhouse-operator
subjects:
  - kind: ServiceAccount
    name: opik-altinity-clickhouse-operator
    namespace: opik
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/Service-clickhouse-operator-metrics.yaml
# Template Parameters:
#
# NAMESPACE=kube-system
# COMMENT=
#
# Setup ClusterIP Service to provide monitoring metrics for Prometheus
# Service would be created in kubectl-specified namespace
# In order to get access outside of k8s it should be exposed as:
# kubectl --namespace prometheus port-forward service/prometheus 9090
# and point browser to localhost:9090
kind: Service
apiVersion: v1
metadata:
  name: opik-altinity-clickhouse-operator-metrics
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
spec:
  ports:
    - port: 8888
      name: clickhouse-metrics
    - port: 9999
      name: operator-metrics
  selector: 
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
---
# Source: opik/charts/mysql/templates/primary/svc-headless.yaml
apiVersion: v1
kind: Service
metadata:
  name: opik-mysql-headless
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  clusterIP: None
  publishNotReadyAddresses: true
  ports:
    - name: mysql
      port: 3306
      targetPort: mysql
  selector:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/name: mysql
    app.kubernetes.io/component: primary
---
# Source: opik/charts/mysql/templates/primary/svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: opik-mysql
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
    app.kubernetes.io/component: primary
spec:
  type: ClusterIP
  sessionAffinity: None
  ports:
    - name: mysql
      port: 3306
      protocol: TCP
      targetPort: mysql
      nodePort: null
  selector:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/name: mysql
    app.kubernetes.io/component: primary
---
# Source: opik/charts/redis/templates/headless-svc.yaml
apiVersion: v1
kind: Service
metadata:
  name: opik-redis-headless
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
spec:
  type: ClusterIP
  clusterIP: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
  selector:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/name: redis
---
# Source: opik/charts/redis/templates/master/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opik-redis-master
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
    app.kubernetes.io/component: master
spec:
  type: ClusterIP
  internalTrafficPolicy: Cluster
  sessionAffinity: None
  ports:
    - name: tcp-redis
      port: 6379
      targetPort: redis
      nodePort: null
  selector:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/name: redis
    app.kubernetes.io/component: master
---
# Source: opik/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opik-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    component: opik-backend
spec:
  type: ClusterIP
  selector:
    component: opik-backend
  ports:
    - port: 8080
      targetPort: 8080
      protocol: TCP
      name: http
    - port: 3003
      targetPort: 3003
      protocol: TCP
      name: backend
---
# Source: opik/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opik-frontend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    component: opik-frontend
spec:
  type: ClusterIP
  selector:
    component: opik-frontend
  ports:
    - port: 5173
      targetPort: 5173
      protocol: TCP
      name: http
---
# Source: opik/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: opik-python-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    component: opik-python-backend
spec:
  type: ClusterIP
  selector:
    component: opik-python-backend
  ports:
    - port: 8000
      targetPort: 8000
      protocol: TCP
      name: http
---
# Source: opik/charts/altinity-clickhouse-operator/templates/generated/Deployment-clickhouse-operator.yaml
# Template Parameters:
#
# NAMESPACE=kube-system
# COMMENT=
# OPERATOR_IMAGE=altinity/clickhouse-operator:0.23.7
# OPERATOR_IMAGE_PULL_POLICY=Always
# METRICS_EXPORTER_IMAGE=altinity/metrics-exporter:0.23.7
# METRICS_EXPORTER_IMAGE_PULL_POLICY=Always
#
# Setup Deployment for clickhouse-operator
# Deployment would be created in kubectl-specified namespace
kind: Deployment
apiVersion: apps/v1
metadata:
  name: opik-altinity-clickhouse-operator
  namespace: opik
  labels: 
    helm.sh/chart: altinity-clickhouse-operator-0.23.7
    app.kubernetes.io/name: altinity-clickhouse-operator
    app.kubernetes.io/instance: opik
    app.kubernetes.io/version: "0.23.7"
    app.kubernetes.io/managed-by: Helm
spec:
  replicas: 1
  selector:
    matchLabels: 
      app.kubernetes.io/name: altinity-clickhouse-operator
      app.kubernetes.io/instance: opik
  template:
    metadata:
      labels: 
        helm.sh/chart: altinity-clickhouse-operator-0.23.7
        app.kubernetes.io/name: altinity-clickhouse-operator
        app.kubernetes.io/instance: opik
        app.kubernetes.io/version: "0.23.7"
        app.kubernetes.io/managed-by: Helm
      annotations:
        
        clickhouse-operator-metrics/port: "9999"
        clickhouse-operator-metrics/scrape: "true"
        prometheus.io/port: "8888"
        prometheus.io/scrape: "true"
        checksum/files: 6cce3a04bc22390eb8418faf3b6c696513460a7a880c68b539805ffdef205cbc
        checksum/confd-files: a22a3d73d06ea04ce191c8524a236cc21ec7acad65da7aadb29b7fcade29a36d
        checksum/configd-files: 490f0543ed663e4bbffb77d49f0e2b49a702773bc2db403d7aa6704b61ecc4e4
        checksum/templatesd-files: b07d059454609b8b3c5015c6ac0c0e7d0dc9fdc8e0e42ff00a7130eea4f3c710
        checksum/usersd-files: b9af758715a1265c176111078f61d2129e5381b1f28cbd7e9da5ace022e3f410
    spec:
      serviceAccountName: opik-altinity-clickhouse-operator
      volumes:
        - name: etc-clickhouse-operator-folder
          configMap:
            name: opik-altinity-clickhouse-operator-files
        - name: etc-clickhouse-operator-confd-folder
          configMap:
            name: opik-altinity-clickhouse-operator-confd-files
        - name: etc-clickhouse-operator-configd-folder
          configMap:
            name: opik-altinity-clickhouse-operator-configd-files
        - name: etc-clickhouse-operator-templatesd-folder
          configMap:
            name: opik-altinity-clickhouse-operator-templatesd-files
        - name: etc-clickhouse-operator-usersd-folder
          configMap:
            name: opik-altinity-clickhouse-operator-usersd-files
      containers:
        - name: altinity-clickhouse-operator
          image: altinity/clickhouse-operator:0.23.7
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: etc-clickhouse-operator-folder
              mountPath: /etc/clickhouse-operator
            - name: etc-clickhouse-operator-confd-folder
              mountPath: /etc/clickhouse-operator/conf.d
            - name: etc-clickhouse-operator-configd-folder
              mountPath: /etc/clickhouse-operator/config.d
            - name: etc-clickhouse-operator-templatesd-folder
              mountPath: /etc/clickhouse-operator/templates.d
            - name: etc-clickhouse-operator-usersd-folder
              mountPath: /etc/clickhouse-operator/users.d
          env:
            # Pod-specific
            # spec.nodeName: ip-172-20-52-62.ec2.internal
            - name: OPERATOR_POD_NODE_NAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
            # metadata.name: clickhouse-operator-6f87589dbb-ftcsf
            - name: OPERATOR_POD_NAME
              valueFrom:
                fieldRef:
                  fieldPath: metadata.name
            # metadata.namespace: kube-system
            - name: OPERATOR_POD_NAMESPACE
              valueFrom:
                fieldRef:
                  fieldPath: metadata.namespace
            # status.podIP: 100.96.3.2
            - name: OPERATOR_POD_IP
              valueFrom:
                fieldRef:
                  fieldPath: status.podIP
            # spec.serviceAccount: clickhouse-operator
            # spec.serviceAccountName: clickhouse-operator
            - name: OPERATOR_POD_SERVICE_ACCOUNT
              valueFrom:
                fieldRef:
                  fieldPath: spec.serviceAccountName
            # Container-specific
            - name: OPERATOR_CONTAINER_CPU_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: requests.cpu
            - name: OPERATOR_CONTAINER_CPU_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: limits.cpu
            - name: OPERATOR_CONTAINER_MEM_REQUEST
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: requests.memory
            - name: OPERATOR_CONTAINER_MEM_LIMIT
              valueFrom:
                resourceFieldRef:
                  containerName: altinity-clickhouse-operator
                  resource: limits.memory
            
          ports:
            - containerPort: 9999
              name: metrics
          resources: 
            {}
          securityContext: 
            {}

      imagePullSecrets: 
        []
      nodeSelector: 
        {}
      affinity: 
        {}
      tolerations: 
        []
      securityContext: 
        {}
---
# Source: opik/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opik-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    component: opik-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opik
      app.kubernetes.io/instance: opik
      app.kubernetes.io/managed-by: Helm
      component: opik-backend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opik
        app.kubernetes.io/instance: opik
        app.kubernetes.io/managed-by: Helm
        component: opik-backend
    spec:
      serviceAccountName: opik-backend
      initContainers:
        - env:
          - name: URL
            value: http://clickhouse-opik-clickhouse:8123
          image: stefanevinance/wait-for-200
          name: wait-for-clickhouse-service
        - name: backend-migrations
          image: "ghcr.io/comet-ml/opik/opik-backend:latest"
          imagePullPolicy: IfNotPresent
          command: ["./run_db_migrations.sh"]
          envFrom:
          - configMapRef:
              name: opik-backend

      containers:

        - name: opik-backend
          securityContext:
            null
          image: "ghcr.io/comet-ml/opik/opik-backend:latest"
          imagePullPolicy: IfNotPresent
          envFrom:
          - configMapRef:
              name: opik-backend
---
# Source: opik/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opik-frontend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    component: opik-frontend
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opik
      app.kubernetes.io/instance: opik
      app.kubernetes.io/managed-by: Helm
      component: opik-frontend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opik
        app.kubernetes.io/instance: opik
        app.kubernetes.io/managed-by: Helm
        component: opik-frontend
    spec:
      serviceAccountName: opik-frontend

      containers:

        - name: opik-frontend
          securityContext:
            null
          image: "ghcr.io/comet-ml/opik/opik-frontend:latest"
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - mountPath: /etc/nginx/conf.d/
              name: opik-frontend-nginx
      volumes:
        - configMap:
            items:
            - key: default.conf
              path: default.conf
            name: opik-frontend-nginx
          name: opik-frontend-nginx
---
# Source: opik/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opik-python-backend
  labels:
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    component: opik-python-backend
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/name: opik
      app.kubernetes.io/instance: opik
      app.kubernetes.io/managed-by: Helm
      component: opik-python-backend
  template:
    metadata:
      labels:
        app.kubernetes.io/name: opik
        app.kubernetes.io/instance: opik
        app.kubernetes.io/managed-by: Helm
        component: opik-python-backend
    spec:
      serviceAccountName: opik-python-backend

      containers:

        - name: opik-python-backend
          securityContext:
            privileged: true
          image: "ghcr.io/comet-ml/opik/opik-python-backend:latest"
          imagePullPolicy: IfNotPresent
---
# Source: opik/charts/mysql/templates/primary/statefulset.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: opik-mysql
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: mysql
    app.kubernetes.io/version: 8.4.1
    helm.sh/chart: mysql-11.1.9
    app.kubernetes.io/component: primary
spec:
  replicas: 1
  podManagementPolicy: ""
  selector:
    matchLabels:
      app.kubernetes.io/instance: opik
      app.kubernetes.io/name: mysql
      app.kubernetes.io/component: primary
  serviceName: opik-mysql
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/configuration: e8ea173e096befb4532171d3dc99a485ae104768b5fbe13cc75a42647e88f4f2
      labels:
        app.kubernetes.io/instance: opik
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: mysql
        app.kubernetes.io/version: 8.4.1
        helm.sh/chart: mysql-11.1.9
        app.kubernetes.io/component: primary
    spec:
      serviceAccountName: opik-mysql
      
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: opik
                    app.kubernetes.io/name: mysql
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      initContainers:
        - name: preserve-logs-symlinks
          image: docker.io/bitnami/mysql:8.4.1-debian-12-r3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          command:
            - /bin/bash
          args:
            - -ec
            - |
              #!/bin/bash

              . /opt/bitnami/scripts/libfs.sh
              # We copy the logs folder because it has symlinks to stdout and stderr
              if ! is_dir_empty /opt/bitnami/mysql/logs; then
                cp -r /opt/bitnami/mysql/logs /emptydir/app-logs-dir
              fi
          volumeMounts:
            - name: empty-dir
              mountPath: /emptydir
      containers:
        - name: mysql
          image: docker.io/bitnami/mysql:8.4.1-debian-12-r3
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: true
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
            seLinuxOptions: {}
            seccompProfile:
              type: RuntimeDefault
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: MYSQL_ROOT_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: opik-mysql
                  key: mysql-root-password
            - name: MYSQL_PORT
              value: "3306"
            - name: MYSQL_DATABASE
              value: "my_database"
          envFrom:
          ports:
            - name: mysql
              containerPort: 3306
          livenessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin status -uroot -p"${password_aux}"
          readinessProbe:
            failureThreshold: 3
            initialDelaySeconds: 5
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin ping -uroot -p"${password_aux}" | grep "mysqld is alive"
          startupProbe:
            failureThreshold: 10
            initialDelaySeconds: 15
            periodSeconds: 10
            successThreshold: 1
            timeoutSeconds: 1
            exec:
              command:
                - /bin/bash
                - -ec
                - |
                  password_aux="${MYSQL_ROOT_PASSWORD:-}"
                  if [[ -f "${MYSQL_ROOT_PASSWORD_FILE:-}" ]]; then
                      password_aux=$(cat "$MYSQL_ROOT_PASSWORD_FILE")
                  fi
                  mysqladmin ping -uroot -p"${password_aux}" | grep "mysqld is alive"
          resources:
            limits:
              cpu: 750m
              ephemeral-storage: 2Gi
              memory: 768Mi
            requests:
              cpu: 500m
              ephemeral-storage: 50Mi
              memory: 512Mi
          volumeMounts:
            - name: data
              mountPath: /bitnami/mysql
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/mysql/conf
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /opt/bitnami/mysql/tmp
              subPath: app-tmp-dir
            - name: empty-dir
              mountPath: /opt/bitnami/mysql/logs
              subPath: app-logs-dir
            - name: custom-init-scripts
              mountPath: /docker-entrypoint-initdb.d
            - name: config
              mountPath: /opt/bitnami/mysql/conf/my.cnf
              subPath: my.cnf
      volumes:
        - name: config
          configMap:
            name: opik-mysql
        - name: custom-init-scripts
          configMap:
            name: opik-mysql-init-scripts
        - name: empty-dir
          emptyDir: {}
  volumeClaimTemplates:
    - metadata:
        name: data
        labels:
          app.kubernetes.io/instance: opik
          app.kubernetes.io/name: mysql
          app.kubernetes.io/component: primary
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: opik/charts/redis/templates/master/application.yaml
apiVersion: apps/v1
kind: StatefulSet
metadata:
  name: opik-redis-master
  namespace: "opik"
  labels:
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: redis
    app.kubernetes.io/version: 7.2.4
    helm.sh/chart: redis-18.19.2
    app.kubernetes.io/component: master
spec:
  replicas: 1
  selector:
    matchLabels:
      app.kubernetes.io/instance: opik
      app.kubernetes.io/name: redis
      app.kubernetes.io/component: master
  serviceName: opik-redis-headless
  updateStrategy:
    type: RollingUpdate
  template:
    metadata:
      labels:
        app.kubernetes.io/instance: opik
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: redis
        app.kubernetes.io/version: 7.2.4
        helm.sh/chart: redis-18.19.2
        app.kubernetes.io/component: master
      annotations:
        checksum/configmap: 74cfc82a26521c398d708f8acac694ad0c5f61af53516446dbb4fabe5de01449
        checksum/health: aff24913d801436ea469d8d374b2ddb3ec4c43ee7ab24663d5f8ff1a1b6991a9
        checksum/scripts: 560c33ff34d845009b51830c332aa05fa211444d1877d3526d3599be7543aaa5
        checksum/secret: 876c54d52db549842125e893956872d26d46359ec6fad3f952192662a9ee09db
    spec:
      
      securityContext:
        fsGroup: 1001
        fsGroupChangePolicy: Always
        supplementalGroups: []
        sysctls: []
      serviceAccountName: opik-redis-master
      automountServiceAccountToken: false
      affinity:
        podAffinity:
          
        podAntiAffinity:
          preferredDuringSchedulingIgnoredDuringExecution:
            - podAffinityTerm:
                labelSelector:
                  matchLabels:
                    app.kubernetes.io/instance: opik
                    app.kubernetes.io/name: redis
                    app.kubernetes.io/component: master
                topologyKey: kubernetes.io/hostname
              weight: 1
        nodeAffinity:
          
      enableServiceLinks: true
      terminationGracePeriodSeconds: 30
      containers:
        - name: redis
          image: docker.io/redis/redis-stack-server:7.2.0-v10
          imagePullPolicy: "IfNotPresent"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
              - ALL
            readOnlyRootFilesystem: false
            runAsGroup: 0
            runAsNonRoot: true
            runAsUser: 1001
            seccompProfile:
              type: RuntimeDefault
          command:
            - /bin/bash
          args:
            - -c
            - /opt/bitnami/scripts/merged-start-scripts/start-master.sh
          env:
            - name: BITNAMI_DEBUG
              value: "false"
            - name: REDIS_REPLICATION_MODE
              value: master
            - name: ALLOW_EMPTY_PASSWORD
              value: "no"
            - name: REDIS_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: opik-redis
                  key: redis-password
            - name: REDIS_TLS_ENABLED
              value: "no"
            - name: REDIS_PORT
              value: "6379"
          ports:
            - name: redis
              containerPort: 6379
          livenessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            # One second longer than command timeout should prevent generation of zombie processes.
            timeoutSeconds: 6
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_liveness_local.sh 5
          readinessProbe:
            initialDelaySeconds: 20
            periodSeconds: 5
            timeoutSeconds: 2
            successThreshold: 1
            failureThreshold: 5
            exec:
              command:
                - sh
                - -c
                - /health/ping_readiness_local.sh 1
          resources:
            limits:
              memory: 1Gi
            requests:
              cpu: 15m
              memory: 105M
          volumeMounts:
            - name: start-scripts
              mountPath: /opt/bitnami/scripts/start-scripts
            - name: health
              mountPath: /health
            - name: redis-data
              mountPath: /data
            - name: config
              mountPath: /opt/bitnami/redis/mounted-etc
            - name: empty-dir
              mountPath: /opt/bitnami/redis/etc/
              subPath: app-conf-dir
            - name: empty-dir
              mountPath: /tmp
              subPath: tmp-dir
            - mountPath: /opt/bitnami/scripts/merged-start-scripts
              name: merged-start-scripts
      volumes:
        - name: start-scripts
          configMap:
            name: opik-redis-scripts
            defaultMode: 0755
        - name: health
          configMap:
            name: opik-redis-health
            defaultMode: 0755
        - name: config
          configMap:
            name: opik-redis-configuration
        - name: empty-dir
          emptyDir: {}
        - configMap:
            defaultMode: 493
            name: bitnami-redis-stack-server-merged
          name: merged-start-scripts
  volumeClaimTemplates:
    - apiVersion: v1
      kind: PersistentVolumeClaim
      metadata:
        name: redis-data
        labels:
          app.kubernetes.io/instance: opik
          app.kubernetes.io/name: redis
          app.kubernetes.io/component: master
      spec:
        accessModes:
          - "ReadWriteOnce"
        resources:
          requests:
            storage: "8Gi"
---
# Source: opik/templates/clickhouseinstallation.yaml
apiVersion: clickhouse.altinity.com/v1
kind: ClickHouseInstallation
metadata:
  name: opik-clickhouse
  namespace: 
  labels:
    component: clickhouse
    app.kubernetes.io/name: opik
    app.kubernetes.io/instance: opik
    app.kubernetes.io/managed-by: Helm
spec:
  defaults:
    templates:
      podTemplate: clickhouse-cluster-pod-template
      serviceTemplate: clickhouse-cluster-svc-template
      volumeClaimTemplate: storage-vc-template
  configuration:
    users:
      opik/password: opik
      opik/access_management: 1
      opik/networks/ip:
        - '127.0.0.1/32'
        - '0.0.0.0/0'
    clusters:
      - name: cluster
        layout:
          shardsCount: 1
          replicasCount: 1
        templates:
          podTemplate: clickhouse-cluster-pod-template
          serviceTemplate: clickhouse-cluster-svc-template
          volumeClaimTemplate: storage-vc-template
  templates:
    podTemplates:
      - name: clickhouse-cluster-pod-template
        spec:
          containers:
          - name: clickhouse
            image: altinity/clickhouse-server:24.3.5.47.altinitystable

    serviceTemplates:
      - name: clickhouse-cluster-svc-lb-template
        metadata:
          annotations:
        spec:
          ports:
            - name: http
              port: 8123
              targetPort: 8123
            - name: tcp
              port: 9000
              targetPort: 9000
          type: LoadBalancer      

    volumeClaimTemplates:
      - name: storage-vc-template
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 50Gi
