apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: phi2
  namespace: model-production
spec:
  predictor:
    minReplicas: 0
    serviceAccountName: sa-s3
    containers:
      - args:
          - --model_name
          - "phi2"
        env:
          - name: STORAGE_URI
            value: s3://model/TheBloke_phi-2-GPTQ_gptq-4bit-32g-actorder_True
        image: 192.168.0.200:45000/serving/vllm:0.0.19
        name: kserve-container
        resources:
          limits:
            cpu: "4"
            memory: 8Gi
            # nvidia.com/gpu: "1"
          requests:
            cpu: "100m"
            memory: 4Gi
            # nvidia.com/gpu: "1"
    nodeSelector:
      kubernetes.io/hostname: ubuntu-gpu-wk